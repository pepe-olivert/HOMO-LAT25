{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pepe/miniconda3/envs/nlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import warnings\n",
    "import random\n",
    "import ast\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_DIM = 128\n",
    "PROB = 0.3\n",
    "DROPOUT = 0.2\n",
    "EMBED_DIM = 768\n",
    "BATCH_SIZE = 16\n",
    "LR = 1e-5\n",
    "WD = 1e-2\n",
    "EPOCHS = 10\n",
    "ACTIVATION = nn.GELU()\n",
    "DATA_AUG = False\n",
    "\n",
    "language = \"ESP\"\n",
    "\n",
    "if language == \"ENG\":\n",
    "    MODEL_NAME = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "    text_col = \"translation\" \n",
    "    emb_col = \"embeddings_ingles\" \n",
    "    par_col = \"paraphrase\" \n",
    "    par_emb_col_1 = \"embedding_paraphrase1\" \n",
    "    par_emb_col_2 = \"embedding_paraphrase2\" \n",
    "    TOKEN_DIM = TOKEN_DIM\n",
    "\n",
    "\n",
    "else: \n",
    "    MODEL_NAME = \"pysentimiento/robertuito-sentiment-analysis\"\n",
    "    text_col = \"post content\"\n",
    "    emb_col = \"embeddings\"\n",
    "    par_col = \"paraphrase_esp\"\n",
    "    par_emb_col_1 = \"embedding_paraphrase_esp1\"\n",
    "    par_emb_col_2 = \"embedding_paraphrase_esp2\"\n",
    "    TOKEN_DIM = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path_to_dataset):\n",
    "    return pd.read_csv(path_to_dataset, index_col = 0)\n",
    "\n",
    "def extract_similar_descriptions(e, embeddings, k):\n",
    "    similarities = cosine_similarity([e], embeddings)[0]\n",
    "    top_indices = np.argsort(similarities)[-k:][::-1]\n",
    "    return top_indices\n",
    "    \n",
    "def compute_embeddings_distance(embedding_str, k, df):\n",
    "    #df = load_dataset(path_to_dataset)\n",
    "\n",
    "    # QUitamos del dataset la misma fila con la que estamos trabajando\n",
    "    df = df[df[emb_col]!= embedding_str]\n",
    "\n",
    "    def parse_embedding(embedding_str):\n",
    "        embedding_str = embedding_str.strip(\"[]\")\n",
    "        return np.array(embedding_str.split(), dtype=np.float32)\n",
    "    \n",
    "    embedding = parse_embedding(embedding_str)\n",
    "    df[emb_col] = df[emb_col].apply(lambda x: parse_embedding(x))\n",
    "\n",
    "    df_pos = df[df[\"label\"]==2]\n",
    "    df_neu = df[df[\"label\"]==1]\n",
    "    df_neg = df[df[\"label\"]==0]\n",
    "\n",
    "    pos_description_embeddings = np.vstack(df_pos[emb_col].values)\n",
    "    pos_nearest_descriptions = extract_similar_descriptions(embedding, pos_description_embeddings, k)\n",
    "    pos_result = df_pos.iloc[pos_nearest_descriptions][text_col]\n",
    "\n",
    "    neu_description_embeddings = np.vstack(df_neu[emb_col].values)\n",
    "    neu_nearest_descriptions = extract_similar_descriptions(embedding, neu_description_embeddings, k)\n",
    "    neu_result = df_neu.iloc[neu_nearest_descriptions][text_col]\n",
    "\n",
    "    neg_description_embeddings = np.vstack(df_neg[emb_col].values)\n",
    "    neg_nearest_descriptions = extract_similar_descriptions(embedding, neg_description_embeddings, k)\n",
    "    neg_result = df_neg.iloc[neg_nearest_descriptions][text_col]\n",
    "    \n",
    "\n",
    "    return pos_result.to_list(), neu_result.to_list(), neg_result.to_list()\n",
    "\n",
    "mapping = {\"NEG\":0, \"NEU\":1, \"POS\":2}\n",
    "\n",
    "def label2int(label):\n",
    "    return mapping[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device count: 1\n",
      "Current device: 0\n",
      "NVIDIA GeForce RTX 4070 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device count:\", torch.cuda.device_count())\n",
    "    print(\"Current device:\", torch.cuda.current_device())\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(torch.cuda.get_device_name(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"../data/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = load_dataset(\"../data/train_paraphrased.csv\")\n",
    "# test_df = load_dataset(\"../data/test_paraphrased.csv\")\n",
    "# val_df = load_dataset(\"../data/val_paraphrased.csv\")\n",
    "# dev_df = load_dataset(\"../data/dev_paraphrased.csv\")\n",
    "# train_df = pd.concat([train_df, dev_df]).reset_index()\n",
    "\n",
    "train_df, test_df = train_test_split(data, test_size=0.2, random_state=RANDOM_SEED, stratify=data['label'])\n",
    "train_df, val_df = train_test_split(train_df, test_size=1/8, random_state=RANDOM_SEED, stratify=train_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_df[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: tensor([0.9873, 0.5916, 3.3694], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_labels),\n",
    "    y=train_labels\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "print(\"Class weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, paraphrases, labels, tokenizer, k, df, type = None, augmentation = False, local_database = train_df):\n",
    "        super().__init__() \n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.k = k\n",
    "        self.df = df\n",
    "        self.local_database = local_database\n",
    "        self.paraphrases = paraphrases\n",
    "        self.type = type\n",
    "        self.augmentation = augmentation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # DATA AUG\n",
    "        if self.type == \"train\" and self.augmentation:\n",
    "            paraphrase = self.paraphrases[idx]\n",
    "            samples = ast.literal_eval(paraphrase)\n",
    "            samples=list(samples)\n",
    "            samples.append(text)\n",
    "            text = random.choice(samples)\n",
    "            index = samples.index(text)\n",
    "\n",
    "\n",
    "            if index == 0:\n",
    "                embedding_str = self.df.loc[self.df[par_col] == paraphrase, par_emb_col_1].values[0]\n",
    "            elif index == 1:\n",
    "                embedding_str = self.df.loc[self.df[par_col] == paraphrase, par_emb_col_2].values[0]\n",
    "            else: \n",
    "                embedding_str = self.df.loc[self.df[text_col] == text, emb_col].values[0]\n",
    "\n",
    "            words = text.split()\n",
    "            if random.random() < PROB:\n",
    "            \n",
    "                if len(words) > 1:\n",
    "                    num_to_remove = max(1, int(0.1 * len(words)))  # Quitar ~10% de las palabras\n",
    "                    indices_to_remove = random.sample(range(len(words)), num_to_remove)\n",
    "                    words = [w for i, w in enumerate(words) if i not in indices_to_remove]\n",
    "                    text = \" \".join(words)\n",
    "\n",
    "                if random.random() < PROB:\n",
    "\n",
    "                    if len(words) > 1:\n",
    "                        random.shuffle(words)\n",
    "                        text = \" \".join(words)\n",
    "\n",
    "        else:\n",
    "            embedding_str = embedding_str = self.df.loc[self.df[text_col] == text, emb_col].values[0]\n",
    "        \n",
    "        # Obtener los textos más similares por clase\n",
    "        pos_texts, neu_texts, neg_texts = compute_embeddings_distance(embedding_str, self.k, self.local_database)\n",
    "        # Tokenizar el texto principal\n",
    "        encoding = self.tokenizer(text, padding=\"max_length\", truncation=True, max_length=TOKEN_DIM, return_tensors=\"pt\")\n",
    "\n",
    "        # Tokenizar los textos más similares\n",
    "        pos_tokens = self.tokenizer(pos_texts, padding=\"max_length\", truncation=True, max_length=TOKEN_DIM, return_tensors=\"pt\")\n",
    "        neu_tokens = self.tokenizer(neu_texts, padding=\"max_length\", truncation=True, max_length=TOKEN_DIM, return_tensors=\"pt\")\n",
    "        neg_tokens = self.tokenizer(neg_texts, padding=\"max_length\", truncation=True, max_length=TOKEN_DIM, return_tensors=\"pt\")\n",
    "        \n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "            \"pos_tokens\": pos_tokens[\"input_ids\"],\n",
    "            \"neu_tokens\": neu_tokens[\"input_ids\"],\n",
    "            \"neg_tokens\": neg_tokens[\"input_ids\"],\n",
    "            \"pos_attention\": pos_tokens[\"attention_mask\"],\n",
    "            \"neu_attention\": neu_tokens[\"attention_mask\"],\n",
    "            \"neg_attention\": neg_tokens[\"attention_mask\"],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "train_dataset = SentimentDataset(\n",
    "    texts=train_df[text_col].tolist(),\n",
    "    labels=train_df[\"label\"].tolist(),\n",
    "    paraphrases=train_df[par_col].tolist(),\n",
    "    tokenizer=tokenizer,\n",
    "    k=1,  # Número de mensajes similares a recuperar\n",
    "    df = train_df,\n",
    "    type = \"train\",\n",
    "    augmentation=DATA_AUG\n",
    ")\n",
    "\n",
    "val_dataset = SentimentDataset(\n",
    "    texts=val_df[text_col].tolist(),\n",
    "    labels=val_df[\"label\"].tolist(),\n",
    "    paraphrases = [],\n",
    "    tokenizer=tokenizer,\n",
    "    k=1,  # Número de mensajes similares a recuperar\n",
    "    df = val_df\n",
    ")\n",
    "\n",
    "test_dataset = SentimentDataset(\n",
    "    texts=test_df[text_col].tolist(),\n",
    "    labels=test_df[\"label\"].tolist(),\n",
    "    paraphrases = [],\n",
    "    tokenizer=tokenizer,\n",
    "    k=1,  # Número de mensajes similares a recuperar\n",
    "    df = test_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1:\n",
      "Text: yo he llegado a pensar que el rechazo, en cuanto a preferencia, contra los `trans` tiene relación con el \"uncanny valley\", y por lo tanto, es algo que responde al subconciente de las personas, difícil de explicar, pero no por eso menos presente o poderoso. eso hace más complicado responder al asunto, en el caso del sub , me doy cuenta de que en general se utiliza mucho maquillaje en la versión final, y claro, el poder del maquillaje es grande, y con ello salta la pregunta de ¿a qué te refieres con atractiva? una cosa que me faltó acotar, es que en las imágenes de ese sub se nota también que hay personas que tenian cierta apariencia de un género distinto antes de la transformación, no hay que olvidar que las disforias se pueden causar por factores genéticos también. yo dudo que seguiría adelante, sabiendo que es una persona `trans`. y esto es hablando del caso en que es indistinguible del caso biológico, el motivo es simple pero un poco pacato, creo que nunca he tenido una relación lo suficientemente pasajera como para que no exista la idea subyacente de que se vuelva algo perduradero, lo más lejos que he llegado en ese ámbito ha sido \"andar\" sin pololear. así que seguramente no soy el mejor ejemplo. pero por ese motivo es que simplemente no es algo que tendría con alguien `trans`, para mi por más transformación que tengan, siguen siendo el sexo biológico de nacimiento, así que chocaría con mi propia sexualidad. en un caso donde no fuera indistinguible del caso biológico creo que es de perogrullo, así que no se si me parecería honestamente atractiva. para mi la pregunta más interesante es qué harían si se enamoraran de un o una `trans`, sin saber que lo es, y luego se enteran. hablo de que ya existieran afectos arraigados. la respuesta fácil es \"le dejaría por haberme mentido\", pero incluso en parejas hetero hay mentiras que se perdonan. ¿qué harían?\n",
      "Label: 1\n",
      "Input IDs: tensor([    0,   560,   723,  5772,   412,  2015,   443,   459,  4158,  4838,\n",
      "          452,  3208,   412, 22440,    16,  1086,   497, 20521, 10806,    40,\n",
      "          838,  4176,   461,   459,  7943,   947,  3923,   948,  3981,  1749,\n",
      "          445,   489,   496,  7391,   442,   964,   443,  6213,   492,  1442,\n",
      "          906,  2364,   413,   529,  7600,  2804,   413,  4883,    16,   583,\n",
      "          464,   489,   774,  1038,  4116,   477, 17148,   970,   774,   834,\n",
      "          588, 10282,  5067,   492,  5921,  1150,   452,   459,  2085,   525,\n",
      "         1442,   916,   474,  2971,  1213,   413,   443,   452,  2587,   475,\n",
      "        11173,   971, 13619,   452,   446,  5228,  9385,   445,  5328,   459,\n",
      "         1322,   525, 13619,   442, 10796,   445,   461,  5806, 11673,   446,\n",
      "         2912,   413,  4686,   792,   516, 22302,   461,  3707, 18215,    33,\n",
      "          531,  1841,   443,   474,  9467,  6314,  6967,   442,   443,   452,\n",
      "          529,  5871,   413,   974,  1442,   475,  2153,     2])\n",
      "Attention Mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Positives: tensor([[    0,   977,   587,  1531,   445,   658,   583,   669,   580, 20031,\n",
      "           468,   464,  1380,   447,  1181,  1380,  1091,  1531,  2496,   774,\n",
      "           464,   442,   648,  3302,   648,  1220,   447,  1181,   461,  5368,\n",
      "           618,   235,   464,   496,  1154,   443,  2381,  1091,   461, 13685,\n",
      "           235,   235,   235,  1531,   445,  4627,   692,  7471,  5285,   648,\n",
      "          3238,   461,   682, 20857,   656,    65,   509,   976,   235,   464,\n",
      "           703,   443, 13452,  1433,   486,   235,  1632,   848, 18203, 18005,\n",
      "           635,   776,   412,   502,  4212,   445,  7877,  7816,   464, 12597,\n",
      "           609,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1]])\n",
      "Neutral: tensor([[    0,  6610,   560,  2381,   496,  2235,   973, 24931,   412,   471,\n",
      "         14424,   485,   464,  1531,   513, 19649,   583,  1477,   593,  2160,\n",
      "           513,  1430,   759,  1141,   475, 24416,   492,   983,   599, 17199,\n",
      "           443,   516,  3204,   669,   848,   489,   497,  3880,   438,   235,\n",
      "           502,   703,  6871,   685,   443,  3304, 13972,  8926,  1070,     2,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1]])\n",
      "Negatives: tensor([[    0,   828,  9110,    61,   865,   974,  9588,   413,   446,   752,\n",
      "           459,  1108,  7576,  1531, 12000,  1835, 12307,   525,  7816,   443,\n",
      "          1215,  3695,  1165,  1842,   529,  1165,   525,  8555,  1531,   446,\n",
      "          1881,    54,   489,   446,  8009,  3695,  1165,  1215,  7816,   520,\n",
      "            54,  2548,   632,   492,  1545,   452,   523,  1186,   509,   601,\n",
      "          1070,   489, 16229,  2952,   477,   489,  2816,  1545,   412,   496,\n",
      "         23364,   235,   235,   445,   461,  5771,   412,   496,   413,   443,\n",
      "          1664,  2267,   471,  1707,  2914,  5587,  3320,   443,   411, 12431,\n",
      "           774,  4482,   489,   459,  1880,   443,  3997,   482,   728,   529,\n",
      "          1165,  1531,   464,  6472, 12796,   413,  1608,  2462,   551,   976,\n",
      "          2921,  7098,  4270,   461,  3591,   413,  1091,   502,   496, 11081,\n",
      "           443,   579,   538, 13482,   442,   443,   529,  5336,  3404,   413,\n",
      "          5747, 18790,   235,   235,   410,   407,   502,     2]])\n",
      "Positives: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "Neutral: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "Negatives: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    sample = train_dataset[i]\n",
    "    print(f\"Sample {i + 1}:\")\n",
    "    print(f\"Text: {train_dataset.texts[i]}\")\n",
    "    print(f\"Label: {sample['labels'].item()}\")\n",
    "    print(f\"Input IDs: {sample['input_ids']}\")\n",
    "    print(f\"Attention Mask: {sample['attention_mask']}\")\n",
    "    print(f\"Positives: {sample['pos_tokens']}\")\n",
    "    print(f\"Neutral: {sample['neu_tokens']}\")\n",
    "    print(f\"Negatives: {sample['neg_tokens']}\")\n",
    "    print(f\"Positives: {sample['pos_attention']}\")\n",
    "    print(f\"Neutral: {sample['neu_attention']}\")\n",
    "    print(f\"Negatives: {sample['neg_attention']}\")\n",
    "    print(\"\\n\" + \"-\"*40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    # Compute the dot products between Q and K, then scale by the square root of the key dimension\n",
    "    d_k = Q.size(-1)\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "\n",
    "    # Apply mask if provided (useful for masked self-attention in transformers)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "    # Softmax to normalize scores, producing attention weights\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Compute the final output as weighted values\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        # Define linear transformations for Q, K, V\n",
    "        self.query = nn.Linear(embed_size, embed_size)\n",
    "        self.key = nn.Linear(embed_size, embed_size)\n",
    "        self.value = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # Generate Q, K, V matrices\n",
    "        Q = self.query(q)\n",
    "        K = self.key(k)\n",
    "        V = self.value(v)\n",
    "        \n",
    "        # Calculate attention using our scaled dot-product function\n",
    "        out, _ = scaled_dot_product_attention(Q, K, V)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at pysentimiento/robertuito-sentiment-analysis and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "class SentimentClassifierWithMultiAttention(nn.Module):\n",
    "    def __init__(self, base_model_name, num_labels=3, num_heads=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.bert = AutoModel.from_pretrained(base_model_name)\n",
    "\n",
    "        self.attention = nn.MultiheadAttention(embed_dim = EMBED_DIM, num_heads = num_heads)\n",
    "\n",
    "\n",
    "        \"\"\"self.linear_intermediate = nn.Linear(EMBED_DIM * 2, EMBED_DIM)\n",
    "        self.activation = ACTIVATION\n",
    "        self.dropout = nn.Dropout(DROPOUT)\"\"\"\n",
    "        \n",
    "\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(EMBED_DIM * 2, EMBED_DIM),\n",
    "            ACTIVATION,\n",
    "            nn.Dropout(DROPOUT),\n",
    "            nn.Linear(EMBED_DIM, EMBED_DIM // 2),\n",
    "            ACTIVATION,\n",
    "            nn.Dropout(DROPOUT),\n",
    "            nn.Linear(EMBED_DIM // 2, EMBED_DIM // 4),\n",
    "            ACTIVATION,\n",
    "            nn.Dropout(DROPOUT)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(EMBED_DIM // 4, num_labels)\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None, pos_tokens=None, neu_tokens=None, neg_tokens=None, pos_attention=None, neu_attention = None, neg_attention = None):\n",
    "        \n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]  # Token CLS\n",
    "        cls_embedding_expanded = cls_embedding.unsqueeze(1)\n",
    "\n",
    "        pos_outputs_0 = self.bert(pos_tokens[:,0,:], attention_mask=pos_attention[:,0,:])\n",
    "        pos_cls_embedding_0 = pos_outputs_0.last_hidden_state[:, 0, :]\n",
    "        pos_cls_embedding_0 = pos_cls_embedding_0.unsqueeze(1)  \n",
    "\n",
    "        pos_cls_embedding = pos_cls_embedding_0\n",
    "\n",
    "        neu_outputs_0 = self.bert(neu_tokens[:,0,:], attention_mask=neu_attention[:,0,:])\n",
    "        neu_cls_embedding_0 = neu_outputs_0.last_hidden_state[:, 0, :]\n",
    "        neu_cls_embedding_0 = neu_cls_embedding_0.unsqueeze(1)  \n",
    "\n",
    "        neu_cls_embedding = neu_cls_embedding_0\n",
    "\n",
    "        neg_outputs_0 = self.bert(neg_tokens[:,0,:], attention_mask=neg_attention[:,0,:])\n",
    "        neg_cls_embedding_0 = neg_outputs_0.last_hidden_state[:, 0, :]\n",
    "        neg_cls_embedding_0 = neg_cls_embedding_0.unsqueeze(1)  \n",
    "\n",
    "        neg_cls_embedding = neg_cls_embedding_0\n",
    "\n",
    "        cls_embedding_expanded = cls_embedding_expanded.float()\n",
    "        pos_cls_embedding = pos_cls_embedding.float()\n",
    "        neg_cls_embedding = neg_cls_embedding.float()\n",
    "        neu_cls_embedding = neu_cls_embedding.float()\n",
    "\n",
    "        \n",
    "        \n",
    "        class_embed = torch.cat([pos_cls_embedding, neg_cls_embedding, neu_cls_embedding])\n",
    "        contextual_emb, _ = self.attention(cls_embedding_expanded, class_embed, class_embed)\n",
    "        contextual_emb = contextual_emb.squeeze(1)\n",
    "\n",
    "        \n",
    "\n",
    "        combined_embedding = torch.cat([\n",
    "            cls_embedding,\n",
    "            contextual_emb\n",
    "        ], dim=-1)\n",
    "        \n",
    "\n",
    "        #logits = self.classifier(self.dropout(self.activation(self.linear_intermediate(combined_embedding))))\n",
    "        logits = self.classifier(self.feedforward(combined_embedding))\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "model = SentimentClassifierWithMultiAttention(MODEL_NAME, num_labels=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congelamos todas las capas menos la última capa de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param in list(model.bert.parameters()):\n",
    "#    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentClassifierWithMultiAttention(\n",
       "  (bert): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(30002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(130, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (attention): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (feedforward): Sequential(\n",
       "    (0): Linear(in_features=1536, out_features=768, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=768, out_features=384, bias=True)\n",
       "    (4): GELU(approximate='none')\n",
       "    (5): Dropout(p=0.2, inplace=False)\n",
       "    (6): Linear(in_features=384, out_features=192, bias=True)\n",
       "    (7): GELU(approximate='none')\n",
       "    (8): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (classifier): Linear(in_features=192, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=data_collator)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=data_collator)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning Context Awareness Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3160\n"
     ]
    }
   ],
   "source": [
    "models_path = \"../checkpoints/\"\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LR, weight_decay=WD) #, weight_decay=WD\n",
    "\n",
    "num_epochs = EPOCHS\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "scheduler = MultiStepLR(optimizer, milestones=[8], gamma=0.1)\n",
    "\n",
    "\n",
    "print(num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 316/3160 [34:06<4:24:32,  5.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10         Train Loss: 1.0532     Val Loss: 1.0385     Macro F1: 0.4398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 632/3160 [1:14:18<3:49:21,  5.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10         Train Loss: 0.9153     Val Loss: 1.0221     Macro F1: 0.4771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 948/3160 [1:55:56<4:05:04,  6.65s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10         Train Loss: 0.7445     Val Loss: 1.1781     Macro F1: 0.5067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 1264/3160 [2:38:09<2:40:55,  5.09s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10         Train Loss: 0.5941     Val Loss: 1.2040     Macro F1: 0.4837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1580/3160 [3:17:19<2:17:24,  5.22s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10         Train Loss: 0.4473     Val Loss: 1.4512     Macro F1: 0.4901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 1896/3160 [3:57:04<1:37:44,  4.64s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10         Train Loss: 0.3271     Val Loss: 1.6678     Macro F1: 0.4852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 2212/3160 [4:35:10<1:21:12,  5.14s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10         Train Loss: 0.2127     Val Loss: 1.9862     Macro F1: 0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 2528/3160 [5:12:29<48:59,  4.65s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10         Train Loss: 0.1627     Val Loss: 2.2409     Macro F1: 0.4782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 2844/3160 [5:49:52<28:43,  5.45s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10         Train Loss: 0.1191     Val Loss: 2.4985     Macro F1: 0.4788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3160/3160 [6:26:38<00:00,  4.74s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10         Train Loss: 0.0775     Val Loss: 2.7453     Macro F1: 0.4675\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "lfunct = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_train_loss = 0 \n",
    "    num_train_batches = 0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        outputs = model(**batch)  # Logits\n",
    "        y_true = batch[\"labels\"]\n",
    "        \n",
    "        loss = lfunct(outputs, y_true)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        #scheduler.step()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Accumulate loss\n",
    "        epoch_train_loss += loss.item()\n",
    "        num_train_batches += 1\n",
    "\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Compute average training loss for the epoch\n",
    "    avg_train_loss = epoch_train_loss / num_train_batches\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # Run validation\n",
    "    model.eval()\n",
    "    epoch_val_loss = 0\n",
    "    num_val_batches = 0\n",
    "\n",
    "    y_preds_val = []\n",
    "    y_trues_val = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            y_true = batch[\"labels\"]\n",
    "\n",
    "            loss = lfunct(outputs, y_true)\n",
    "            epoch_val_loss += loss.item()\n",
    "            num_val_batches += 1\n",
    "\n",
    "            y_preds_val.extend(torch.argmax(outputs, dim=-1).cpu().tolist())\n",
    "            y_trues_val.extend(batch[\"labels\"].cpu().tolist())\n",
    "\n",
    "    y_preds_val = np.array(y_preds_val)\n",
    "    y_trues_val = np.array(y_trues_val)\n",
    "    f1_val = f1_score(y_trues_val, y_preds_val, average='macro')\n",
    "\n",
    "    avg_val_loss = epoch_val_loss / num_val_batches\n",
    "    val_losses.append(avg_val_loss)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs:<10} Train Loss: {avg_train_loss:<10.4f} Val Loss: {avg_val_loss:<10.4f} Macro F1: {f1_val:.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), f\"../checkpoints/{epoch + 1}_context.pth\")\n",
    "    # torch.save(model, f\"../checkpoints/{epoch + 1}_full_context_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot Training vs Validation Loss\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(train_losses, label=\"Training Loss\", color=\"blue\", alpha=0.7)\n",
    "plt.plot(val_losses, label=\"Validation Loss\", color=\"red\", alpha=0.7)\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(721,) (721,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         NEG       0.50      0.52      0.51       244\n",
      "         NEU       0.65      0.67      0.66       406\n",
      "         POS       0.28      0.20      0.23        71\n",
      "\n",
      "    accuracy                           0.57       721\n",
      "   macro avg       0.48      0.46      0.47       721\n",
      "weighted avg       0.56      0.57      0.57       721\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.eval()\n",
    "\n",
    "y_pred_val = []\n",
    "y_true_val = []\n",
    "with torch.no_grad(): \n",
    "    for batch in val_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        y_pred_val.extend(torch.argmax(outputs, dim=-1).tolist())\n",
    "        y_true_val.extend(batch[\"labels\"].tolist())\n",
    "\n",
    "y_pred_val = np.array(y_pred_val)\n",
    "y_true_val = np.array(y_true_val)\n",
    "print(y_pred_val.shape, y_true_val.shape)\n",
    "print(classification_report(y_true_val, y_pred_val, target_names=mapping.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1442,) (1442,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         NEG       0.52      0.53      0.53       487\n",
      "         NEU       0.66      0.68      0.67       812\n",
      "         POS       0.36      0.27      0.31       143\n",
      "\n",
      "    accuracy                           0.59      1442\n",
      "   macro avg       0.52      0.50      0.50      1442\n",
      "weighted avg       0.58      0.59      0.59      1442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.eval()\n",
    "\n",
    "y_pred_test = []\n",
    "y_true_test = []\n",
    "with torch.no_grad(): \n",
    "    for batch in test_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        y_pred_test.extend(torch.argmax(outputs, dim=-1).tolist())\n",
    "        y_true_test.extend(batch[\"labels\"].tolist())\n",
    "\n",
    "y_pred_test = np.array(y_pred_test)\n",
    "y_true_test = np.array(y_true_test)\n",
    "print(y_pred_test.shape, y_true_test.shape)\n",
    "print(classification_report(y_true_test, y_pred_test, target_names=mapping.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at pysentimiento/robertuito-sentiment-analysis and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SentimentClassifierWithMultiAttention(\n",
       "  (bert): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(30002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(130, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (attention): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (feedforward): Sequential(\n",
       "    (0): Linear(in_features=1536, out_features=768, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=768, out_features=384, bias=True)\n",
       "    (4): GELU(approximate='none')\n",
       "    (5): Dropout(p=0.2, inplace=False)\n",
       "    (6): Linear(in_features=384, out_features=192, bias=True)\n",
       "    (7): GELU(approximate='none')\n",
       "    (8): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (classifier): Linear(in_features=192, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load(\"../checkpoints/4_context.pth\", map_location=torch.device(device)) \n",
    "model = SentimentClassifierWithMultiAttention(MODEL_NAME, num_labels=3)\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sample(model, tokenizer, sample_text, label, embedding_str):\n",
    "    \"\"\"\n",
    "    Predict the sentiment of a sample text\n",
    "    \"\"\"\n",
    "\n",
    "    pos_texts, neu_texts, neg_texts = compute_embeddings_distance(embedding_str, 1, train_df)\n",
    "    encoding = tokenizer(sample_text, padding=\"max_length\", truncation=True, max_length=TOKEN_DIM, return_tensors=\"pt\")\n",
    "\n",
    "    # Tokenizar los textos más similares\n",
    "    pos_tokens = tokenizer(pos_texts, padding=\"max_length\", truncation=True, max_length=TOKEN_DIM, return_tensors=\"pt\")\n",
    "    neu_tokens = tokenizer(neu_texts, padding=\"max_length\", truncation=True, max_length=TOKEN_DIM, return_tensors=\"pt\")\n",
    "    neg_tokens = tokenizer(neg_texts, padding=\"max_length\", truncation=True, max_length=TOKEN_DIM, return_tensors=\"pt\")\n",
    "    \n",
    "\n",
    "    inputs = {\n",
    "        \"input_ids\": encoding[\"input_ids\"],\n",
    "        \"attention_mask\": encoding[\"attention_mask\"],\n",
    "        \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "        \"pos_tokens\": pos_tokens[\"input_ids\"].unsqueeze(1),\n",
    "        \"neu_tokens\": neu_tokens[\"input_ids\"].unsqueeze(1),\n",
    "        \"neg_tokens\": neg_tokens[\"input_ids\"].unsqueeze(1),\n",
    "        \"pos_attention\": pos_tokens[\"attention_mask\"].unsqueeze(1),\n",
    "        \"neu_attention\": neu_tokens[\"attention_mask\"].unsqueeze(1),\n",
    "        \"neg_attention\": neg_tokens[\"attention_mask\"].unsqueeze(1),\n",
    "    }\n",
    "    \n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    logits = outputs\n",
    "    probabilities = torch.softmax(logits, dim=-1)\n",
    "    \n",
    "    predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
    "    \n",
    "    \n",
    "    return predicted_class, probabilities[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(721,) (721,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         NEG       0.54      0.55      0.55       244\n",
      "         NEU       0.68      0.61      0.64       406\n",
      "         POS       0.21      0.34      0.26        71\n",
      "\n",
      "    accuracy                           0.56       721\n",
      "   macro avg       0.48      0.50      0.48       721\n",
      "weighted avg       0.59      0.56      0.57       721\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "y_pred_val = []\n",
    "y_true_val = []\n",
    "with torch.no_grad(): \n",
    "    for batch in val_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        y_pred_val.extend(torch.argmax(outputs, dim=-1).tolist())\n",
    "        y_true_val.extend(batch[\"labels\"].tolist())\n",
    "\n",
    "y_pred_val = np.array(y_pred_val)\n",
    "y_true_val = np.array(y_true_val)\n",
    "print(y_pred_val.shape, y_true_val.shape)\n",
    "print(classification_report(y_true_val, y_pred_val, target_names=mapping.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1442,) (1442,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         NEG       0.54      0.53      0.54       487\n",
      "         NEU       0.69      0.61      0.65       812\n",
      "         POS       0.28      0.48      0.36       143\n",
      "\n",
      "    accuracy                           0.57      1442\n",
      "   macro avg       0.50      0.54      0.51      1442\n",
      "weighted avg       0.60      0.57      0.58      1442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "y_pred_test = []\n",
    "y_true_test = []\n",
    "with torch.no_grad(): \n",
    "    for batch in test_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        y_pred_test.extend(torch.argmax(outputs, dim=-1).tolist())\n",
    "        y_true_test.extend(batch[\"labels\"].tolist())\n",
    "\n",
    "y_pred_test = np.array(y_pred_test)\n",
    "y_true_test = np.array(y_true_test)\n",
    "print(y_pred_test.shape, y_true_test.shape)\n",
    "print(classification_report(y_true_test, y_pred_test, target_names=mapping.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Incompatible dimension for X and Y matrices: X.shape[1] == 384 while Y.shape[1] == 512",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m input_emb_p2 \u001b[38;5;241m=\u001b[39m test_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding_paraphrase2\u001b[39m\u001b[38;5;124m\"\u001b[39m][i]\n\u001b[1;32m     16\u001b[0m real_label \u001b[38;5;241m=\u001b[39m test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m][i]\n\u001b[0;32m---> 18\u001b[0m predicted_label, probabilities \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_embedding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m predicted_label1, probabilities1 \u001b[38;5;241m=\u001b[39m predict_sample(model, tokenizer, paraphrase_text1, input_label, input_emb_p1)\n\u001b[1;32m     20\u001b[0m predicted_label2, probabilities2 \u001b[38;5;241m=\u001b[39m predict_sample(model, tokenizer, paraphrase_text2, input_label, input_emb_p2)\n",
      "Cell \u001b[0;32mIn[25], line 6\u001b[0m, in \u001b[0;36mpredict_sample\u001b[0;34m(model, tokenizer, sample_text, label, embedding_str)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict_sample\u001b[39m(model, tokenizer, sample_text, label, embedding_str):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    Predict the sentiment of a sample text\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     pos_texts, neu_texts, neg_texts \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_embeddings_distance\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m tokenizer(sample_text, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39mTOKEN_DIM, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Tokenizar los textos más similares\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 27\u001b[0m, in \u001b[0;36mcompute_embeddings_distance\u001b[0;34m(embedding_str, k, df)\u001b[0m\n\u001b[1;32m     24\u001b[0m df_neg \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     26\u001b[0m pos_description_embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack(df_pos[emb_col]\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[0;32m---> 27\u001b[0m pos_nearest_descriptions \u001b[38;5;241m=\u001b[39m \u001b[43mextract_similar_descriptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_description_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m pos_result \u001b[38;5;241m=\u001b[39m df_pos\u001b[38;5;241m.\u001b[39miloc[pos_nearest_descriptions][text_col]\n\u001b[1;32m     30\u001b[0m neu_description_embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack(df_neu[emb_col]\u001b[38;5;241m.\u001b[39mvalues)\n",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m, in \u001b[0;36mextract_similar_descriptions\u001b[0;34m(e, embeddings, k)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract_similar_descriptions\u001b[39m(e, embeddings, k):\n\u001b[0;32m----> 5\u001b[0m     similarities \u001b[38;5;241m=\u001b[39m \u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43me\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      6\u001b[0m     top_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(similarities)[\u001b[38;5;241m-\u001b[39mk:][::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m top_indices\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    214\u001b[0m         )\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    226\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/sklearn/metrics/pairwise.py:1741\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m   1695\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[1;32m   1696\u001b[0m \n\u001b[1;32m   1697\u001b[0m \u001b[38;5;124;03mCosine similarity, or the cosine kernel, computes similarity as the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1737\u001b[0m \u001b[38;5;124;03m       [0.57..., 0.81...]])\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;66;03m# to avoid recursive import\u001b[39;00m\n\u001b[0;32m-> 1741\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_pairwise_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1743\u001b[0m X_normalized \u001b[38;5;241m=\u001b[39m normalize(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m Y:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/sklearn/metrics/pairwise.py:229\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[0;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, ensure_all_finite, ensure_2d, copy)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    222\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecomputed metric requires shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    223\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(n_queries, n_indexed). Got (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    224\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m indexed.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    225\u001b[0m         )\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m ensure_2d \u001b[38;5;129;01mand\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# Only check the number of features if 2d arrays are enforced. Otherwise,\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# validation is left to the user for custom metrics.\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    230\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible dimension for X and Y matrices: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX.shape[1] == \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m while Y.shape[1] == \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    232\u001b[0m     )\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, Y\n",
      "\u001b[0;31mValueError\u001b[0m: Incompatible dimension for X and Y matrices: X.shape[1] == 384 while Y.shape[1] == 512"
     ]
    }
   ],
   "source": [
    "y_pred_test = []\n",
    "y_true_test = []\n",
    "\n",
    "for i in range(len(test_df)):\n",
    "    input_text = test_df[\"translation\"][i]\n",
    "    input_label = test_df[\"label\"][i]\n",
    "    input_embedding = test_df[\"embeddings_ingles\"][i]    \n",
    "    # Get paraphrased samples\n",
    "    samples = ast.literal_eval(test_df[\"paraphrase\"][i])\n",
    "    samples=list(samples)\n",
    "    paraphrase_text1 = samples[0]\n",
    "    paraphrase_text2 = samples[1]\n",
    "    input_emb_p1 = test_df[\"embedding_paraphrase1\"][i]\n",
    "    input_emb_p2 = test_df[\"embedding_paraphrase2\"][i]\n",
    "\n",
    "    real_label = test_df['label'][i]\n",
    "\n",
    "    predicted_label, probabilities = predict_sample(model, tokenizer, input_text, input_label, input_embedding)\n",
    "    predicted_label1, probabilities1 = predict_sample(model, tokenizer, paraphrase_text1, input_label, input_emb_p1)\n",
    "    predicted_label2, probabilities2 = predict_sample(model, tokenizer, paraphrase_text2, input_label, input_emb_p2)\n",
    "\n",
    "    if predicted_label != predicted_label1 and predicted_label != predicted_label2 and predicted_label1 != predicted_label2:\n",
    "        labels = [predicted_label, predicted_label1, predicted_label2]\n",
    "        label1_score = max(probabilities)\n",
    "        label2_score = max(probabilities1)\n",
    "        label3_score = max(probabilities2)\n",
    "        scores = {label1_score: predicted_label, label2_score: predicted_label1, label3_score: predicted_label2}\n",
    "        final_label = max(scores.keys())\n",
    "        predicted_label = scores[final_label]\n",
    "    else:\n",
    "        # Get the most frequent label\n",
    "        predicted_label = max(predicted_label, predicted_label1, predicted_label2, key=[predicted_label, predicted_label1, predicted_label2].count)\n",
    "        \n",
    "    y_pred_test.append(predicted_label)\n",
    "    y_true_test.append(real_label)\n",
    "\n",
    "y_pred_test = np.array(y_pred_test)\n",
    "y_true_test = np.array(y_true_test)\n",
    "\n",
    "print(classification_report(y_true_test, y_pred_test, target_names=mapping.keys()))\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true_test, y_pred_test)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=mapping.keys())\n",
    "disp.plot(cmap=plt.cm.Blues)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
